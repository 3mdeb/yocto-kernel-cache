From d2f8bdb8818bf2b83ac75b6b5e8428be61242d19 Mon Sep 17 00:00:00 2001
From: Rik van Riel <riel@redhat.com>
Date: Mon, 18 Aug 2014 13:45:38 -0700
Subject: [PATCH 28/28] numa,sched: fix load_to_imbalanced logic inversion

commit 1662867a9b2574bfdb9d4e97186aa131218d7210 upstream

This function is supposed to return true if the new load imbalance is
worse than the old one.  It didn't.  I can only hope brown paper bags
are in style.

Now things converge much better on both the 4 node and 8 node systems.

I am not sure why this did not seem to impact specjbb performance on the
4 node system, which is the system I have full-time access to.

This bug was introduced recently, with commit e63da03639cc ("sched/numa:
Allow task switch if load imbalance improves")

Signed-off-by: Rik van Riel <riel@redhat.com>
Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
Signed-off-by: Yang Shi <yang.shi@windriver.com>
Signed-off-by: Bruce Ashfield <bruce.ashfield@windriver.com>
---
 kernel/sched/fair.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 80e70e17eeff..97a91b8850a9 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1137,7 +1137,7 @@ static bool load_too_imbalanced(long orig_src_load, long orig_dst_load,
 	old_imb = orig_dst_load * 100 - orig_src_load * env->imbalance_pct;
 
 	/* Would this change make things worse? */
-	return (old_imb > imb);
+	return (imb > old_imb);
 }
 
 /*
-- 
1.8.1.2

